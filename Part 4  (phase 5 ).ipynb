{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 5\n",
    "     Model Building (Score 80 + 180 Extra credits):\n",
    "\n",
    " Each student must solve\n",
    "\n",
    "    . Supervised Learning: Build a sentiment analysis model to predict positive and negative classes (Score 40)\n",
    "    . Unsupervised Learning: Build a clustering model consisting of 2 clusters based on positive and negative reviews (Score 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\parentpoller.py:116: UserWarning: Parent poll failed.  If the frontend dies,\n",
      "                the kernel may be left running.  Please let us know\n",
      "                about your system (bitness, Python, etc.) at\n",
      "                ipython-dev@scipy.org\n",
      "  ipython-dev@scipy.org\"\"\")\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#getting the data \n",
    "# importing pickled data for supervised and unsupervised \n",
    "sup_df=pd.read_pickle('pickledfiles/cl_fs_imdb_df.pkl') \n",
    "sup_df=sup_df[['feature_selected','label']] # cleaned and feature selected reviews \n",
    "#sup_df.shape \n",
    "\n",
    "unsup_df=pd.read_pickle('pickledfiles/unsup_df.pkl') # not cleaned or fs reviews \n",
    "#unsup_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as unsup_df is not cleaned would have to clean it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clinic_all_clean (mess):\n",
    "    \n",
    "    import string\n",
    "    import re\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk import pos_tag\n",
    "   \n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.stem.porter import PorterStemmer \n",
    "\n",
    "    \n",
    "    '''this function does the following:\n",
    "        > it makes word lower case\n",
    "        > it removes word containg numbers\n",
    "        1. it removes words inside <> like <br>\n",
    "        2. it removes punctuations \n",
    "        3. it removes stopwords \n",
    "        4. it stemmers\n",
    "        5. it lemmatize\n",
    "    '''\n",
    "    \n",
    "  \n",
    "    \n",
    "    mess = mess.lower()                 # make lowercase\n",
    "    \n",
    "    mess = re.sub(r'[^\\x00-\\x7F]+','', mess) # remove non ascii character \n",
    "    \n",
    "    mess = re.sub('\\(.*?\\)', '', mess) # removes words like latin words (El Monte de las brujas)  that are inside ( )\n",
    "    \n",
    "    mess = re.sub('\\<.*?\\>', '', mess) # removes words like br that are inside < >\n",
    "    \n",
    "    mess = re.sub('\\w*\\d\\w*', '', mess) # removes words containg  numbers\n",
    "    \n",
    "    nopunc = [x for x in mess if x not in string.punctuation] # remove all punc\n",
    "     # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    stpwrd = [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "    \n",
    "    # now it stemmers \n",
    "    stem = PorterStemmer()\n",
    "    stem_w =[stem.stem(wrd) for wrd in stpwrd]\n",
    "    \n",
    "    \n",
    "    # now its lemmatizes\n",
    "    wnl = WordNetLemmatizer()\n",
    "    lem_txt=[wnl.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v'] else wnl.lemmatize(i) for i,j in pos_tag(stem_w)]\n",
    "    \n",
    "    return ' '.join(lem_txt) # removing join returns list of words aka bag of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(mess):\n",
    "    import string\n",
    "\n",
    "    return mess.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### takes a while (where while ~ an hour or so. but its one time as i pickle it )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsup_df['cleaned_review']=unsup_df.review.apply(clinic_all_clean)\n",
    "\n",
    "unsup_df.to_pickle('pickledfiles/clean_unsup_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_unsup_df=pd.read_pickle('pickledfiles/clean_unsup_df.pkl')# cleaned unsup dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>cleaned_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I admit, the great majority of films released ...</td>\n",
       "      <td>admit great major film releas say dozen major ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Take a low budget, inexperienced actors doubli...</td>\n",
       "      <td>take low budget inexperienc actor doubl produc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Everybody has seen 'Back To The Future,' right...</td>\n",
       "      <td>everybodi see back futur right whether like mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Doris Day was an icon of beauty in singing and...</td>\n",
       "      <td>dori day icon beauti sing act warm voic geniu ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>After a series of silly, fun-loving movies, 19...</td>\n",
       "      <td>seri silli funlov movi big year dori day year ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  I admit, the great majority of films released ...   \n",
       "1  Take a low budget, inexperienced actors doubli...   \n",
       "2  Everybody has seen 'Back To The Future,' right...   \n",
       "3  Doris Day was an icon of beauty in singing and...   \n",
       "4  After a series of silly, fun-loving movies, 19...   \n",
       "\n",
       "                                      cleaned_review  \n",
       "0  admit great major film releas say dozen major ...  \n",
       "1  take low budget inexperienc actor doubl produc...  \n",
       "2  everybodi see back futur right whether like mo...  \n",
       "3  dori day icon beauti sing act warm voic geniu ...  \n",
       "4  seri silli funlov movi big year dori day year ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl_unsup_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning: Build a sentiment analysis model to predict positive and negative classes (Score 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing data\n",
    "x = sup_df.feature_selected\n",
    "# encode the target strings\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "y = le.fit_transform(sup_df.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000 10000 50000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "msg_train, msg_test, label_train, label_test = \\\n",
    "train_test_split(x, y, test_size=0.2)#80:20 pareto \n",
    "\n",
    "print(len(msg_train), len(msg_test), len(msg_train) + len(msg_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultinomialNB\n",
    "## creating pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.86      0.86      4996\n",
      "          1       0.86      0.86      0.86      5004\n",
      "\n",
      "avg / total       0.86      0.86      0.86     10000\n",
      " time taken: 3.118654727935791\n"
     ]
    }
   ],
   "source": [
    "pipeline1 = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])\n",
    "\n",
    "start_time = time.time()\n",
    "pipeline1.fit(msg_train,label_train)\n",
    "cal_time1 = (time.time()- start_time)\n",
    "\n",
    "predictions1 = pipeline1.predict(msg_test)\n",
    "print(classification_report(predictions1,label_test),'time taken:',cal_time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8618"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the accuracy of the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy1 = accuracy_score(label_test, predictions1)\n",
    "accuracy1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.86'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score1=classification_report(predictions1,label_test).split( )[-2]\n",
    "f1_score1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra Credits:**\n",
    "        \n",
    "        . Supervised Learning: Compare the performance of different machine learning models, at least 2 (Score 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline5 = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier',  LogisticRegression()), \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.90      0.89      4786\n",
      "          1       0.91      0.88      0.89      5214\n",
      "\n",
      "avg / total       0.89      0.89      0.89     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "pipeline5.fit(msg_train,label_train)\n",
    "cal_time5 = (time.time()- start_time)\n",
    "predictions5 = pipeline5.predict(msg_test)\n",
    "\n",
    "print(classification_report(predictions5,label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.889"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the accuracy of the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy5 = accuracy_score(label_test, predictions5)\n",
    "accuracy5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.89'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score5=classification_report(predictions5,label_test).split( )[-2]\n",
    "f1_score5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline9 = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier',  SGDClassifier()),  # train on TF-IDF vectors w/  DecisionTreeClassifier\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.90      0.88      4792\n",
      "          1       0.91      0.87      0.89      5208\n",
      "\n",
      "avg / total       0.89      0.89      0.89     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "pipeline9.fit(msg_train,label_train)\n",
    "cal_time9 = (time.time()- start_time)\n",
    "predictions9 = pipeline9.predict(msg_test)\n",
    "\n",
    "print(classification_report(predictions9,label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8864"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the accuracy of the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy9 = accuracy_score(label_test, predictions9)\n",
    "accuracy9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.89'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score9=classification_report(predictions9,label_test).split( )[-2]\n",
    "f1_score9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### comparing the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>naive bayes</td>\n",
       "      <td>0.8618</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.118654727935791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.89</td>\n",
       "      <td>3.0687906742095947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>0.8864</td>\n",
       "      <td>0.89</td>\n",
       "      <td>3.5285329818725586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         model accuracy f1_score                time\n",
       "0                  naive bayes   0.8618     0.86   3.118654727935791\n",
       "1          Logistic Regression    0.889     0.89  3.0687906742095947\n",
       "2  Stochastic Gradient Descent   0.8864     0.89  3.5285329818725586"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alist = [('naive bayes',accuracy1,f1_score1,cal_time1),\n",
    "          \n",
    "          ('Logistic Regression',accuracy5,f1_score5,cal_time5),\n",
    "          \n",
    "          ('Stochastic Gradient Descent',accuracy9,f1_score9,cal_time9),\n",
    "          ]\n",
    "    \n",
    "pd.DataFrame(np.array(alist),columns=['model','accuracy','f1_score','time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning:\n",
    "Build a clustering model consisting of 2 clusters based on positive and negative reviews (Score 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phase2_cleaning(mess):\n",
    "    import string \n",
    "    added_stopwords = ['film', 'movi', 'one', 'see', 'make', 'like', 'get', 'show', 'charact']\n",
    "        # remove addition stopwords that we had created in Part2 \n",
    "    stpwrd = [word for word in mess.split() if word.lower() not in added_stopwords]\n",
    "    return stpwrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cl_unsup_df=pd.read_pickle('pickledfiles/clean_unsup_df.pkl')# cleaned unsup dataframe\n",
    "msg_train=cl_unsup_df.cleaned_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#vectorizer\n",
    "vectorizer=TfidfVectorizer(analyzer=phase2_cleaning)\n",
    "X=vectorizer.fit_transform(msg_train)\n",
    "true_k = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster id : 0:\n",
      "stori\n",
      "great\n",
      "time\n",
      "love\n",
      "good\n",
      "play\n",
      "well\n",
      "scene\n",
      "also\n",
      "watch\n",
      "cluster id : 1:\n",
      "watch\n",
      "bad\n",
      "good\n",
      "realli\n",
      "think\n",
      "time\n",
      "would\n",
      "even\n",
      "dont\n",
      "go\n"
     ]
    }
   ],
   "source": [
    "#model\n",
    "km=KMeans(n_clusters=true_k,n_init=3,max_iter=100)\n",
    "\n",
    "km.fit(X)\n",
    "\n",
    "#looking at clusters \n",
    "\n",
    "centroid=km.cluster_centers_.argsort()[:,::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(true_k):\n",
    "    print('cluster id : %d:' %i),\n",
    "    for j in centroid[i,:10]:\n",
    "        print('%s'% terms[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction\n",
      "[1]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction\")\n",
    "\n",
    "Y = vectorizer.transform(['bad movie dont watch'])\n",
    "prediction = km.predict(Y)\n",
    "print(prediction)\n",
    "\n",
    "Y = vectorizer.transform(['great movie loved it '])\n",
    "prediction = km.predict(Y)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical clustering¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgglomerativeClustering(affinity='euclidean', compute_full_tree='auto',\n",
       "            connectivity=None, linkage='ward', memory=None, n_clusters=2,\n",
       "            pooling_func=<function mean at 0x000001EEDA517BF8>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data subset\n",
    "lk=msg_train[:1000]\n",
    "#vectorizer\n",
    "cv = CountVectorizer()\n",
    "jk=cv.fit(lk)\n",
    "zz=jk.transform(lk)\n",
    "\n",
    "#sparse to dense\n",
    "ss=zz.toarray()\n",
    "\n",
    "#model\n",
    "sg=AgglomerativeClustering()\n",
    "sg.fit(ss)\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Divide the data into 4 clusters to enable finding more classes. Analyse each cluster and try to find the correct label for the new cluster. Repeat clustering until 4 new labels can be found, other than the original labels (positive and negative) (Score 50)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#vectorizer\n",
    "vectorizer=TfidfVectorizer(analyzer=phase2_cleaning)\n",
    "X=vectorizer.fit_transform(msg_train)\n",
    "true_k = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster id : 0:\n",
      "great\n",
      "stori\n",
      "time\n",
      "good\n",
      "play\n",
      "love\n",
      "scene\n",
      "well\n",
      "also\n",
      "watch\n",
      "cluster id : 1:\n",
      "funni\n",
      "comedi\n",
      "laugh\n",
      "watch\n",
      "joke\n",
      "good\n",
      "great\n",
      "time\n",
      "realli\n",
      "think\n",
      "cluster id : 2:\n",
      "realli\n",
      "think\n",
      "watch\n",
      "good\n",
      "stori\n",
      "would\n",
      "go\n",
      "time\n",
      "peopl\n",
      "end\n",
      "cluster id : 3:\n",
      "bad\n",
      "act\n",
      "watch\n",
      "worst\n",
      "even\n",
      "ever\n",
      "good\n",
      "wast\n",
      "dont\n",
      "time\n"
     ]
    }
   ],
   "source": [
    "#model\n",
    "km=KMeans(n_clusters=true_k,n_init=3,max_iter=100)\n",
    "\n",
    "km.fit(X)\n",
    "\n",
    "#looking at clusters \n",
    "\n",
    "centroid=km.cluster_centers_.argsort()[:,::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(true_k):\n",
    "    print('cluster id : %d:' %i),\n",
    "    for j in centroid[i,:10]:\n",
    "        print('%s'% terms[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  now that we divided data into 4 clusters lets analyze cluster and decide classes\n",
    "\n",
    "i could go through manually but there wont be anything tangible to backup my claim so trying something crazy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from sklearn import cluster\n",
    "import re\n",
    "\n",
    "def create_polarity_vector(all_adjectives):\n",
    "    # NLTK Vader sentiment analyzer\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    #Taking compound score as one feature, (compund score = -ve indicates negative sentiments)\n",
    "    #(compund score = +ve indicates positive sentiments, ie compound value is normalization b/w +ve and -ve)\n",
    "    feature_vector=[1 if sid.polarity_scores(i)['compound']>=0 else -1 for i in all_adjectives]\n",
    "    \n",
    "    return feature_vector\n",
    "\n",
    "\n",
    "\n",
    "# utility function to extract all adjectives from text document\n",
    "def extract_adjectives(text):\n",
    "    tokenized_text= nltk.word_tokenize(text) # tokenizes sentences to words \n",
    "    tagged_text = nltk.pos_tag(tokenized_text) # creates tag for each word \n",
    "    all_adjectives = list({i[0] for i in tagged_text if i[1] == 'JJ'}) #JJ: adjective or numeral, ordinal\n",
    "    return all_adjectives\n",
    "\n",
    "# Cleaning up text  # does not lemma and steming as it may hinder getting proper tag \n",
    "def clean_text(text):    \n",
    "    \n",
    "    text = text.lower()                 # make lowercase\n",
    "    \n",
    "    text = re.sub('\\<.*?\\>', '', text) # removes words like br that are inside < >\n",
    "    \n",
    "    text = re.sub('\\w*\\d\\w*', '', text) # removes words containg  numbers\n",
    "    \n",
    "    #stopword removal\n",
    "    stop_word=list(stopwords.words('english'))\n",
    "    toke=list(text.split(' '))\n",
    "    text = ' '.join([i for i in toke if i not in stop_word])\n",
    "    \n",
    "    #punctutaion removal\n",
    "    t_lator=str.maketrans('','',string.punctuation)\n",
    "    text=text.translate(t_lator)\n",
    "    remove_digits = str.maketrans('', '', string.digits)\n",
    "    text = text.translate(remove_digits)    \n",
    "    \n",
    "    #removing special symbol\n",
    "    for i in '“”—':\n",
    "        text = text.replace(i, ' ')\n",
    "        \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166391"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroid[3].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "clu1 = [] # we can play with amount of words we need from each cluster however selecting very high number diminishes the difference\n",
    "\n",
    "for j in centroid[1,:800]:\n",
    "        #print('%s'% terms[j])\n",
    "        clu1.append(terms[j])\n",
    "        \n",
    "        \n",
    "clu2 = []\n",
    "for j in centroid[2,:800]:\n",
    "        #print('%s'% terms[j])\n",
    "        clu2.append(terms[j])\n",
    "        \n",
    "\n",
    "clu3 = []\n",
    "for j in centroid[3,:800]:\n",
    "        #print('%s'% terms[j])\n",
    "        clu3.append(terms[j])\n",
    "        \n",
    "clu0 = []\n",
    "for j in centroid[0,:800]:\n",
    "        #print('%s'% terms[j])\n",
    "        clu0.append(terms[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_0=extract_adjectives(' '.join(clu0))\n",
    "adj_1=extract_adjectives(' '.join(clu1))\n",
    "adj_2=extract_adjectives(' '.join(clu2))\n",
    "adj_3=extract_adjectives(' '.join(clu3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_polarity_vector(all_adjectives):\n",
    "    # NLTK Vader sentiment analyzer\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    #Taking compound score as one feature, (compund score = -ve indicates negative sentiments)\n",
    "    #(compund score = +ve indicates positive sentiments, ie compound value is normalization b/w +ve and -ve)\n",
    "    feature_vector=[1 if sid.polarity_scores(i)['compound']>=0 else -1 for i in all_adjectives]\n",
    "    \n",
    "    return feature_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.175, 'neu': 0.552, 'pos': 0.272, 'compound': 0.9607}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sid.polarity_scores(' '.join(adj_0))# ++ positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.211, 'neu': 0.532, 'pos': 0.257, 'compound': 0.8591}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sid.polarity_scores(' '.join(adj_1))# + positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.236, 'neu': 0.518, 'pos': 0.246, 'compound': -0.3561}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sid.polarity_scores(' '.join(adj_2))# - negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.204, 'neu': 0.616, 'pos': 0.18, 'compound': -0.765}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sid.polarity_scores(' '.join(adj_3)) # -- negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#conclusion \n",
    ".note: each time you run code the cluster name or id may interchange however values remain somewhat same \n",
    "\n",
    ">**postive:**\n",
    "    \n",
    "    .cluster 0 is most positive ++\n",
    "    .cluster 1 is positive +\n",
    "\n",
    ">**negative:**\n",
    "    \n",
    "    .cluster 2 is most negative --\n",
    "    .cluster 3 is negative -\n",
    "    \n",
    "    \n",
    "\n",
    "#positive and negative are the main clusters \n",
    ">  .clusters (3,0) are sub-cluster of positive \n",
    "\n",
    ">   .clusters(2,1) are sub_ cluster of negative "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active Learning: Cluster the training dataset and try and find the genre. Manually annotate the cluster and then try to find the labels in the new testing dataset. (Score 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# performed it in new notebook i.e Part5(active)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
