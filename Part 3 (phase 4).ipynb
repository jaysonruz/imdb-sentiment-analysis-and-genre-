{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Phase 4 \n",
    "# Classification Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### importing Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_selected</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>work way chill classic movi pack collect witch...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>whoever write screenplay movi obvious never co...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>although properli warn actual sat watch movi p...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>possibl worst film within genr exist announc c...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>depict man neglect son movi dont get wrong con...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    feature_selected     label\n",
       "0  work way chill classic movi pack collect witch...  negative\n",
       "1  whoever write screenplay movi obvious never co...  negative\n",
       "2  although properli warn actual sat watch movi p...  negative\n",
       "3  possibl worst film within genr exist announc c...  negative\n",
       "4  depict man neglect son movi dont get wrong con...  positive"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_pickle('pickledfiles/cl_fs_imdb_df.pkl')# clean and feature selected df\n",
    "df=df[['feature_selected','label']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since we using cleaded and feature selected  data already no point using computaion so just used .split() and comment out rest \n",
    "\n",
    "def text_process(mess):\n",
    "    import string\n",
    "    from nltk.corpus import stopwords\n",
    "#     \"\"\"\n",
    "#     Takes in a string of text, then performs the following:\n",
    "#     1. Remove all punctuation\n",
    "#     2. Remove all stopwords\n",
    "#     3. Returns a list of the cleaned text\n",
    "#     \"\"\"\n",
    "#     # Check characters to see if they are in punctuation\n",
    "#     nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "#     # Join the characters again to form the string.\n",
    "#     nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    return mess.split() #[word for word in nopunc.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### getting data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.feature_selected\n",
    "# encode the target strings\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "y = le.fit_transform(df.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000 15000 50000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "msg_train, msg_test, label_train, label_test = \\\n",
    "train_test_split(x, y, test_size=0.3)\n",
    "\n",
    "print(len(msg_train), len(msg_test), len(msg_train) + len(msg_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### types of algorithms\n",
    "\n",
    "\n",
    "   ***parametric***\n",
    "    \n",
    "    .logistic Regression \n",
    "    .Naive Bayes Classifier\n",
    "    .Support Vector Machines\n",
    "    .Decision Trees\n",
    "    .Random Forest\n",
    "    .Nearest Neighbor\n",
    "    .Stochastic Gradient Descent\n",
    "   ***non_parametric***\n",
    "   \n",
    "    .kMeans clustering\n",
    "    .Hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import time\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.86      0.86      7495\n",
      "          1       0.86      0.86      0.86      7505\n",
      "\n",
      "avg / total       0.86      0.86      0.86     15000\n",
      " time taken: 4.361330270767212\n"
     ]
    }
   ],
   "source": [
    "pipeline1 = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])\n",
    "start_time = time.time()\n",
    "pipeline1.fit(msg_train,label_train)\n",
    "cal_time1 = (time.time()- start_time)\n",
    "\n",
    "predictions1 = pipeline1.predict(msg_test)\n",
    "print(classification_report(predictions1,label_test),'time taken:',cal_time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.86'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score1=classification_report(predictions1,label_test).split( )[-2]\n",
    "f1_score1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8594"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the accuracy of the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy1 = accuracy_score(label_test, predictions1)\n",
    "accuracy1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.73      0.73      7478\n",
      "          1       0.73      0.73      0.73      7522\n",
      "\n",
      "avg / total       0.73      0.73      0.73     15000\n",
      " time taken: 91.04072880744934\n"
     ]
    }
   ],
   "source": [
    "pipeline2 = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n",
    "    #('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier',  DecisionTreeClassifier()),  \n",
    "])\n",
    "start_time = time.time()\n",
    "pipeline2.fit(msg_train,label_train)\n",
    "cal_time2 = (time.time()- start_time)\n",
    "predictions2 = pipeline2.predict(msg_test)\n",
    "\n",
    "print(classification_report(predictions2,label_test),'time taken:',cal_time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7270666666666666"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the accuracy of the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy2 = accuracy_score(label_test, predictions2)\n",
    "accuracy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.73'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score2=classification_report(predictions2,label_test).split( )[-2]\n",
    "f1_score2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.73      0.78      8530\n",
      "          1       0.70      0.81      0.75      6470\n",
      "\n",
      "avg / total       0.78      0.77      0.77     15000\n",
      " time taken: 23.6417818069458\n"
     ]
    }
   ],
   "source": [
    "pipeline3 = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier',  RandomForestClassifier()),\n",
    "]) \n",
    "start_time = time.time()\n",
    "pipeline3.fit(msg_train,label_train)\n",
    "cal_time3 = (time.time()- start_time)\n",
    "predictions3 = pipeline3.predict(msg_test)\n",
    "\n",
    "print(classification_report(predictions3,label_test),'time taken:',cal_time3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7656"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the accuracy of the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy3 = accuracy_score(label_test, predictions3)\n",
    "accuracy3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.77'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score3=classification_report(predictions3,label_test).split( )[-2]\n",
    "f1_score3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline4 = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n",
    "    #('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier',  SVC()),  \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### caution fitting below model is time consuming ,i have pickled the model just in case you want to save time  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "\n",
    "# pipeline4.fit(msg_train,label_train)\n",
    "# predictions4 = pipeline4.predict(msg_test)\n",
    "# cal_time4 = (time.time()- start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    " import pickle\n",
    "# # save the model to disk\n",
    "filename = 'pickledmodels/finalized_SVC_model.sav'\n",
    "filename1 = 'pickledmodels/finalized_SVC_prediction.pred'\n",
    "# pickle.dump(pipeline4, open(filename, 'wb'))\n",
    "# pickle.dump(predictions4, open(filename1, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the model from disk\n",
    "# #import pickle\n",
    "pipeline4 = pickle.load(open(filename, 'rb'))\n",
    "prediction4= pickle.load(open(filename1, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.85      0.79      6576\n",
      "          1       0.87      0.77      0.82      8424\n",
      "\n",
      "avg / total       0.81      0.81      0.81     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictions4,label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8058666666666666"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the accuracy of the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy4 = accuracy_score(label_test, predictions4)\n",
    "accuracy4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.81'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score4=classification_report(predictions4,label_test).split( )[-2]\n",
    "f1_score4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline5 = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier',  LogisticRegression()), \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.90      0.89      7315\n",
      "          1       0.90      0.88      0.89      7685\n",
      "\n",
      "avg / total       0.89      0.89      0.89     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "pipeline5.fit(msg_train,label_train)\n",
    "cal_time5 = (time.time()- start_time)\n",
    "predictions5 = pipeline5.predict(msg_test)\n",
    "\n",
    "print(classification_report(predictions5,label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8868666666666667"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the accuracy of the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy5 = accuracy_score(label_test, predictions5)\n",
    "accuracy5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.89'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score5=classification_report(predictions5,label_test).split( )[-2]\n",
    "f1_score5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline9 = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier',  SGDClassifier()),  # train on TF-IDF vectors w/  DecisionTreeClassifier\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.90      0.88      7088\n",
      "          1       0.91      0.86      0.89      7912\n",
      "\n",
      "avg / total       0.88      0.88      0.88     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "pipeline9.fit(msg_train,label_train)\n",
    "cal_time9 = (time.time()- start_time)\n",
    "predictions9 = pipeline9.predict(msg_test)\n",
    "\n",
    "print(classification_report(predictions9,label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8829333333333333"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the accuracy of the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy9 = accuracy_score(label_test, predictions9)\n",
    "accuracy9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.88'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score9=classification_report(predictions9,label_test).split( )[-2]\n",
    "f1_score9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NON parametric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline6 = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier',  KNeighborsClassifier()), \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#time comsuming cell below use pickled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pipeline6.fit(msg_train,label_train)\n",
    "\n",
    "#predictions6 = pipeline6.predict(msg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    " import pickle\n",
    "# # save the model to disk\n",
    "filename = 'pickledmodels/finalized_kNeighbours_model.sav'\n",
    "filename1 = 'pickledmodels/finalized_SVC_prediction.pred'\n",
    "# pickle.dump(predictions6, open(filename1, 'wb'))\n",
    "# pickle.dump(pipeline6, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "pipeline6 = pickle.load(open(filename, 'rb'))\n",
    "predictions6 = pickle.load(open(filename1, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.44      0.49      0.47      6729\n",
      "          1       0.55      0.50      0.52      8271\n",
      "\n",
      "avg / total       0.50      0.50      0.50     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictions6,label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4954"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the accuracy of the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy6 = accuracy_score(label_test, predictions6)\n",
    "accuracy6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.50'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score6=classification_report(predictions6,label_test).split( )[-2]\n",
    "f1_score6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster id : 0:\n",
      "film\n",
      "one\n",
      "movi\n",
      "make\n",
      "like\n",
      "see\n",
      "show\n",
      "get\n",
      "charact\n",
      "stori\n",
      "cluster id : 1:\n",
      "movi\n",
      "see\n",
      "like\n",
      "watch\n",
      "make\n",
      "one\n",
      "bad\n",
      "good\n",
      "realli\n",
      "think\n"
     ]
    }
   ],
   "source": [
    " \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#vectorizer\n",
    "vectorizer=TfidfVectorizer()\n",
    "X=vectorizer.fit_transform(msg_train)\n",
    "\n",
    "#model\n",
    "km=KMeans(n_clusters=2,n_init=3,max_iter=100)\n",
    "\n",
    "km.fit(X)\n",
    "\n",
    "#looking at clusters \n",
    "\n",
    "centroid=km.cluster_centers_.argsort()[:,::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(2):\n",
    "    print('cluster id : %d:' %i),\n",
    "    for j in centroid[i,:10]:\n",
    "        print('%s'% terms[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline7 = Pipeline([\n",
    "#     ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n",
    "#     ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "#     ('classifier',  KMeans(n_clusters=2,n_init=3,max_iter=100) )\n",
    "#     ])\n",
    "\n",
    "# ####  time consuming cell below use pickled model\n",
    "\n",
    "# # pipeline7.fit(msg_train)\n",
    "\n",
    "# import pickle\n",
    "# # save the model to disk\n",
    "# filename = 'pickledmodels/finalized_kmean_model.sav'\n",
    "# #pickle.dump(pipeline7, open(filename, 'wb'))\n",
    " \n",
    "\n",
    "# # load the model from disk\n",
    "# pipeline7 = pickle.load(open(filename, 'rb'))\n",
    " \n",
    "\n",
    "# predictions7 = pipeline7.predict(msg_test)\n",
    "\n",
    "# print(classification_report(predictions7,label_test))\n",
    "\n",
    "# # check the accuracy of the model\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# accuracy = accuracy_score(label_test, predictions7)\n",
    "# accuracy\n",
    "\n",
    "# f1_score7=classification_report(predictions7,label_test).split( )[-2]\n",
    "# f1_score7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical clustering\n",
    "MemoryError when ran on entire data\n",
    " so selected 10000 of them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.20      0.58      0.30       178\n",
      "          1       0.85      0.50      0.63       822\n",
      "\n",
      "avg / total       0.73      0.51      0.57      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#data subset\n",
    "lk=msg_train[:1000]\n",
    "#vectorizer\n",
    "cv = CountVectorizer()\n",
    "jk=cv.fit(lk)\n",
    "zz=jk.transform(lk)\n",
    "\n",
    "#sparse to dense\n",
    "ss=zz.toarray()\n",
    "\n",
    "#model\n",
    "sg=AgglomerativeClustering()\n",
    "sg.fit(ss)\n",
    "\n",
    "\n",
    "print(classification_report(sg.labels_,label_test[:1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summerizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>naive bayes</td>\n",
       "      <td>0.8594</td>\n",
       "      <td>0.86</td>\n",
       "      <td>4.361330270767212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.7270666666666666</td>\n",
       "      <td>0.73</td>\n",
       "      <td>91.04072880744934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8868666666666667</td>\n",
       "      <td>0.89</td>\n",
       "      <td>4.318443059921265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Support Vector Machines</td>\n",
       "      <td>0.8058666666666666</td>\n",
       "      <td>0.81</td>\n",
       "      <td>1837.2541205883026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.7656</td>\n",
       "      <td>0.77</td>\n",
       "      <td>23.6417818069458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>0.8829333333333333</td>\n",
       "      <td>0.88</td>\n",
       "      <td>2.0854201316833496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>K Neighbor</td>\n",
       "      <td>0.4954</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         model            accuracy f1_score  \\\n",
       "0                  naive bayes              0.8594     0.86   \n",
       "1       DecisionTreeClassifier  0.7270666666666666     0.73   \n",
       "2          Logistic Regression  0.8868666666666667     0.89   \n",
       "3      Support Vector Machines  0.8058666666666666     0.81   \n",
       "4                Random Forest              0.7656     0.77   \n",
       "5  Stochastic Gradient Descent  0.8829333333333333     0.88   \n",
       "6                   K Neighbor              0.4954     0.50   \n",
       "\n",
       "                 time  \n",
       "0   4.361330270767212  \n",
       "1   91.04072880744934  \n",
       "2   4.318443059921265  \n",
       "3  1837.2541205883026  \n",
       "4    23.6417818069458  \n",
       "5  2.0854201316833496  \n",
       "6                   -  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alist = [('naive bayes',accuracy1,f1_score1,cal_time1),\n",
    "          ('DecisionTreeClassifier',accuracy2,f1_score2,cal_time2),\n",
    "          ('Logistic Regression',accuracy5,f1_score5,cal_time5),\n",
    "          ('Support Vector Machines',accuracy4,f1_score4,cal_time4),\n",
    "          ('Random Forest',accuracy3,f1_score3,cal_time3),\n",
    "          ('Stochastic Gradient Descent',accuracy9,f1_score9,cal_time9),\n",
    "          ('K Neighbor',accuracy6,f1_score6,'-')]\n",
    "    \n",
    "pd.DataFrame(np.array(alist),columns=['model','accuracy','f1_score','time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#from the above algorithms it can be observed that for independent and dependent variable , **parametric are more accurate** as well as **less time** consuming relative to **non-parametric**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this completes our phase 4\n",
    "lets move on to **phase 5** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
